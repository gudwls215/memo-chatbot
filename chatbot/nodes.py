"""LangGraph ì±—ë´‡ ë…¸ë“œ êµ¬í˜„ - MCPToolkit ì‚¬ìš©"""
import os
import sys
import asyncio
from typing import Literal
from contextlib import AsyncExitStack
from langchain_openai import ChatOpenAI
from langchain_core.messages import ToolMessage
from dotenv import load_dotenv
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from mcp.client.sse import sse_client
from langchain_mcp import MCPToolkit

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# MCP ì—°ê²° ëª¨ë“œ ì„¤ì •
MCP_MODE = os.getenv("MCP_MODE", "stdio")  # "stdio" ë˜ëŠ” "sse"
MCP_SERVER_URL = os.getenv("MCP_SERVER_URL", "http://localhost:8001/sse")

# MCP ì„œë²„ ê²½ë¡œ (stdio ëª¨ë“œìš©)
server_script = os.path.abspath(
    os.path.join(os.path.dirname(__file__), '..', 'mcp-server', 'server.py')
)

# Python ì‹¤í–‰ íŒŒì¼ ê²½ë¡œ
python_cmd = sys.executable

# ì „ì—­ ë³€ìˆ˜
mcp_toolkit = None
mcp_session = None
exit_stack = None
tools = []
model = None


async def initialize_mcp_client():
    """MCP í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ë° ë„êµ¬ ë¡œë“œ"""
    global mcp_toolkit, mcp_session, exit_stack, tools, model
    
    if mcp_toolkit is not None:
        return
    
    # AsyncExitStackìœ¼ë¡œ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬
    exit_stack = AsyncExitStack()
    await exit_stack.__aenter__()
    
    if MCP_MODE == "sse":
        # SSE ëª¨ë“œ: ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ MCP ì„œë²„ì— ì—°ê²°
        print(f"ğŸ”— SSE ëª¨ë“œë¡œ MCP ì„œë²„ì— ì—°ê²° ì¤‘... ({MCP_SERVER_URL})")
        read_stream, write_stream = await exit_stack.enter_async_context(
            sse_client(MCP_SERVER_URL)
        )
    else:
        # stdio ëª¨ë“œ: MCP ì„œë²„ë¥¼ subprocessë¡œ ì‹¤í–‰
        print("ğŸš€ stdio ëª¨ë“œë¡œ MCP ì„œë²„ ì‹œì‘ ì¤‘...")
        server_params = StdioServerParameters(
            command=python_cmd,
            args=[server_script],
            env=None
        )
        
        read_stream, write_stream = await exit_stack.enter_async_context(
            stdio_client(server_params)
        )
    
    # ì„¸ì…˜ ìƒì„± ë° ì´ˆê¸°í™”
    mcp_session = await exit_stack.enter_async_context(
        ClientSession(read_stream, write_stream)
    )
    await mcp_session.initialize()
    
    # MCPToolkit ìƒì„± ë° ì´ˆê¸°í™”
    mcp_toolkit = MCPToolkit(session=mcp_session)
    await mcp_toolkit.initialize()
    
    # ë„êµ¬ ê°€ì ¸ì˜¤ê¸°
    tools = mcp_toolkit.get_tools()
    
    # ëª¨ë¸ ì´ˆê¸°í™” (ë„êµ¬ ë°”ì¸ë”©)
    model = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0
    ).bind_tools(tools)
    
    mode_text = "SSE ì„œë²„" if MCP_MODE == "sse" else "ë‚´ì¥ ì„œë²„"
    print(f"âœ… MCP {mode_text} ì—°ê²° ì™„ë£Œ! ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬: {[t.name for t in tools]}")


async def cleanup_mcp_client():
    """MCP í´ë¼ì´ì–¸íŠ¸ ì •ë¦¬"""
    global exit_stack
    
    if exit_stack is not None:
        await exit_stack.__aexit__(None, None, None)
        exit_stack = None


async def call_model(state):
    """LLM í˜¸ì¶œ ë…¸ë“œ"""
    global model
    
    # MCP í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì´ˆê¸°í™”
    if model is None:
        await initialize_mcp_client()
    
    messages = state["messages"]
    response = await model.ainvoke(messages)
    return {"messages": [response]}


def should_continue(state) -> Literal["continue", "end"]:
    """ë„êµ¬ í˜¸ì¶œ í•„ìš” ì—¬ë¶€ ê²°ì •"""
    messages = state["messages"]
    last_message = messages[-1]
    
    # ë„êµ¬ í˜¸ì¶œì´ ìˆìœ¼ë©´ ê³„ì†, ì—†ìœ¼ë©´ ì¢…ë£Œ
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "continue"
    return "end"


async def call_tools(state):
    """ë„êµ¬ ì‹¤í–‰ ë…¸ë“œ - MCP í´ë¼ì´ì–¸íŠ¸ë¥¼ í†µí•´ ì‹¤í–‰"""
    global tools
    
    messages = state["messages"]
    last_message = messages[-1]
    
    tool_messages = []
    
    # ë„êµ¬ í˜¸ì¶œ ì‹¤í–‰
    for tool_call in last_message.tool_calls:
        tool_name = tool_call["name"]
        tool_args = tool_call["args"]
        
        # MCP ë„êµ¬ ì°¾ê¸°
        selected_tool = None
        for t in tools:
            if t.name == tool_name:
                selected_tool = t
                break
        
        if selected_tool:
            try:
                # MCPë¥¼ í†µí•´ ë„êµ¬ ì‹¤í–‰
                result = await selected_tool.ainvoke(tool_args)
                tool_messages.append(
                    ToolMessage(
                        content=str(result),
                        tool_call_id=tool_call["id"]
                    )
                )
            except Exception as e:
                tool_messages.append(
                    ToolMessage(
                        content=f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                        tool_call_id=tool_call["id"]
                    )
                )
    
    return {"messages": tool_messages}
